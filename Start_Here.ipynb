{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "682fa012-c6ec-4850-b6e0-5f0323366574",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\"line-height: 1.6;\">\n",
    "\n",
    "<h2 style=\"fond-weight: 600;\"><strong>Grid World Reinforcement Learning</h2>\n",
    "\n",
    "<h4 style=\"text=align: center; font-weight: 400; font-size: 20px; font-style: italic;\">Model-Based vs. Model-Free Reinforcement Learning in a Stochastic GridWorld: Sample Efficiency and Robustness</h4>\n",
    "\n",
    "<p style=\"text-align: center; margin-top: 15px; font-size: 20px;\">Felipe S. Campoverde</p>\n",
    "\n",
    "<p  style=\"text-align: center; font-size: 18px; \">\n",
    "CS 4824 Machine Learning &nbsp; . &nbsp;\n",
    "Instructor: Dr. Ming Jin &nbsp; . &nbsp;\n",
    "Date: 11/20/2025\n",
    "</p>\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c833e6df-f4c8-43e1-97ab-8019eb07f9f7",
   "metadata": {},
   "source": [
    "## **Problem Definition**\n",
    "\n",
    "This project investigates how an agent can learn effective behavior purely via trial–and–error in\n",
    "a compact 2D environment. The student will design a stochastic GridWorld with walls, pits, and\n",
    "wind (transition noise) and compare model-free RL (Q-learning, SARSA) against model-based RL\n",
    "via Dyna-Q, which interleaves learning a one-step model with planning updates.\n",
    "\n",
    "**Motivation & Significance**. GridWorlds isolate core RL phenomena, exploration, boot-\n",
    "strapping, on- vs. off-policy learning without heavy computation. Understanding when planning\n",
    "helps and how hyperparameters affect sample efficiency provides practical guidance for scaling RL\n",
    "to richer problems.\n",
    "\n",
    "**Research Questions.**\n",
    "1. How do planning steps (**K**) in **Dyna-Q** affect sample efficiency (return vs. environment steps)\n",
    "under stochastic dynamics?\n",
    "2. How sensitive are **Q-learning**, **SARSA**, and **Dyna-Q** to exploration schedules ( ε-greedy\n",
    "with/without decay) and learning rates?\n",
    "3. How robust are learned policies to layout changes (added obstacles) and transition noise (wind\n",
    "strength)?\n",
    "4. Does prioritized sweeping further improve efficiency compared to vanilla **Dyna-Q**?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd84be2a-59d5-48f4-9761-e8a61a985c83",
   "metadata": {},
   "source": [
    "## **Literature Review**\n",
    "Foundational work establishes the methods to be compared: Q-learning provides off-policy TD\n",
    "control with convergence guarantees in tabular settings. SARSA offers an on-policy counterpart\n",
    "with different exploration–exploitation behavior. Dyna integrates learning and planning by\n",
    "updating from both real experience and a learned model; prioritized sweeping improves planning\n",
    "by focusing backups where they matter most. Sutton and Barto synthesize these approaches.\n",
    "The project addresses a practical gap: a controlled, reproducible comparison of model-free vs.\n",
    "model-based methods under stochasticity and layout shift, with thorough ablations on planning\n",
    "budget and exploration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70cc38e-02a2-4b2b-b794-29af108f8712",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "**Environment.**  \n",
    "A configurable GridWorld of size N × M with state s = (i, j) and actions A = {↑, →, ↓, ←}.\n",
    "\n",
    "**Rewards:** step = \\(-0.01\\), goal = \\(+1.0\\), pit = \\(-1.0\\) (terminal).  \n",
    "**Wind:** with probability \\( p_w \\), the intended move is rotated left/right before execution.  \n",
    "Multiple layouts (base map + variants) support robustness tests.  \n",
    "**API:** `reset() → s0`, `step(a) → (s’, r, done, info)`, `render()`.\n",
    "\n",
    "---\n",
    "\n",
    "### Algorithms (NumPy, from scratch)\n",
    "\n",
    "- **Q-learning (off-policy):**\n",
    "\n",
    "  $$\n",
    "  Q(s, a) \\leftarrow Q(s, a) + \\alpha [r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)]\n",
    "  $$\n",
    "\n",
    "- **SARSA(0) (on-policy):**\n",
    "\n",
    "  $$\n",
    "  Q(s, a) \\leftarrow Q(s, a) + \\alpha [r + \\gamma Q(s', a') - Q(s, a)], \\quad a' \\sim \\varepsilon\\text{-greedy}\n",
    "  $$\n",
    "\n",
    "- **Dyna-Q (model-based):**  \n",
    "  Learn a one-step model $\\hat{P}(s'|s,a)$ and $\\hat{r}(s,a)$.  \n",
    "  After each real step, perform $K$ planning backups by sampling past $(s,a)$ pairs  \n",
    "  and applying the Q-learning target using model-predicted outcomes.\n",
    "\n",
    "---\n",
    "\n",
    "**Exploration:** $\\varepsilon$-greedy with fixed and decayed $\\varepsilon$; compare schedules.\n",
    "**Ablations:** $K \\in \\{0, 5, 20, 50\\}$, $\\varepsilon \\in \\{0.1, 0.3\\}$ with/without decay,  \n",
    "$\\gamma \\in \\{0.90, 0.99\\}$, wind $p_w \\in \\{0.0, 0.1, 0.2\\}$.\n",
    "\n",
    "**Experimental Design & Metrics**\n",
    "Learning curves (average return per episode), **sample efficiency**  \n",
    "(area under return curve vs. environment steps), success rate and steps-to-goal (median),  \n",
    "robustness to wind and layout shift, and stability across 10 random seeds.  \n",
    "Visualizations include greedy-policy arrows, Q-value heatmaps, and state-visit distributions.\n",
    "\n",
    "**Justification.**  \n",
    "Dyna-Q often accelerates learning when models are accurate; comparing across controlled stochasticity isolates when planning helps and its computational trade-offs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65539ef4-0c70-4a4b-b7ee-57ed9e08002c",
   "metadata": {},
   "source": [
    "## Data & Resources\n",
    "\n",
    "No external data are required; all experience is generated by the simulator. The implementation\n",
    "uses Python, NumPy, Pandas (logging), Matplotlib (plots/animation), and optional Pygame for\n",
    "real-time rendering in Jupyter. CPU-only execution is sufficient (seconds to minutes per sweep).\n",
    "Reproducibility will be ensured with fixed seeds, configuration files for layouts/hyperparameters,\n",
    "and saving results as CSV + figures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86b49d2-8c19-4ca9-829e-3e09738f3e96",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Expected Outcomes\n",
    "\n",
    "Dyna-Q is expected to achieve higher returns with fewer real environment steps than purely model-free methods, particularly at moderate wind. SARSA may exhibit safer behavior under high stochasticity due to on-policy updates, while Q-learning may converge faster in low-noise settings. The project will summarize trade-offs among planning budget, exploration schedule, and robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093efa22-63b7-46ea-8521-d2b210b1c74c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<style>\n",
    "    .button {\n",
    "        background-color: #3b3b3b;\n",
    "        color: white;\n",
    "        padding: 25px 60px;\n",
    "        border: none;\n",
    "        border-radius: 12px;\n",
    "        cursor: pointer;\n",
    "        font-size: 30px;\n",
    "        transition: background-color 0.3s ease;\n",
    "    }\n",
    "\n",
    "    .button:hover {\n",
    "        background-color: #45a049;\n",
    "        transform: scale(1.05);\n",
    "    }\n",
    "    \n",
    "</style>\n",
    "\n",
    "<div style=\" text-align: center; margin-top:20px;\">\n",
    "  <a href=\"./notebooks/00_RL.ipynb\">\n",
    "    <button class=\"button\">\n",
    "      Next: Project Details ➡️\n",
    "    </button>\n",
    "  </a>\n",
    "  \n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
