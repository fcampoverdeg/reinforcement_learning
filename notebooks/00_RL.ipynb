{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41b8fd0c-2555-4a14-ab5d-4eaab2fc4e01",
   "metadata": {},
   "source": [
    "# Welcome to the Reinforcement Learning Project\n",
    "\n",
    "This notebook serves as a quick **guide and orientation** for anyone opening this repository for the first time.\n",
    "It explains what the project is about, how the files are organized, and how to get started running experiments in **JupyterLab**.\n",
    "\n",
    "\n",
    "\n",
    "By: Felipe Campoverde\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45ee031-0f1c-4b17-bab6-da68a0d27e95",
   "metadata": {},
   "source": [
    "## **Important**\n",
    "### Run the code below before anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2efbf361-648d-48a4-897d-1006b6febec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m[ok]\u001b[0m using src at: /home/houndsito/Documents/Development/github/fcampoverdeg/reinforcement_learning/src\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "\u001b[92mEnvironment setup complete!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Robust bootstrap: make `src/` importable from any notebook location.\n",
    "Finds the project root by searching for src/rl_capstone/__init__.py.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from IPython import get_ipython\n",
    "\n",
    "GREEN = \"\\033[92m\"; YELLOW = \"\\033[93m\"; RED = \"\\033[91m\"; RESET = \"\\033[0m\"\n",
    "\n",
    "def add_project_src(pkg_name: str = \"rl_capstone\") -> Path:\n",
    "    here = Path.cwd()\n",
    "    for base in [here, *here.parents]:\n",
    "        marker = base / \"src\" / pkg_name / \"__init__.py\"\n",
    "        if marker.exists():\n",
    "            src_path = (base / \"src\").resolve()\n",
    "            if str(src_path) not in sys.path:\n",
    "                sys.path.insert(0, str(src_path))  # highest priority\n",
    "            print(f\"{GREEN}[ok]{RESET} using src at: {src_path}\")\n",
    "            return src_path\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not locate src/{pkg_name}/__init__.py starting from {here} upwards.\"\n",
    "    )\n",
    "\n",
    "try:\n",
    "    src_path = add_project_src(\"rl_capstone\")\n",
    "\n",
    "    # Auto-reload edited modules in src/ without manual re-imports\n",
    "    ip = get_ipython()\n",
    "    if ip:\n",
    "        ip.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "        ip.run_line_magic(\"autoreload\", \"2\")\n",
    "\n",
    "    # Quick smoke test\n",
    "    from rl_capstone import GridWorld, WorldSettings\n",
    "    _ = GridWorld(WorldSettings())\n",
    "    print(f\"{GREEN}Environment setup complete!{RESET}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"{RED}Environment setup failed:{RESET}\\n{e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1de282-f9a0-452e-ae2b-82d63479a313",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Project Overview\n",
    "\n",
    "Reinforcement Learning (RL) is a branch of Machine Learning that focuses on **how agents learn to make decisions** through interacting with an environment.\n",
    "The agent receives **rewards or penalties** based on its actions and gradually learns a policy that **maximizes cumulative reward** over time.\n",
    "\n",
    "In this project, the student implemented and compared several RL algorithms using a **custome GridWorld environment**:\n",
    "\n",
    "- **Q-Learning** - model-free, off-policy learning.\n",
    "- **SARSA** - model-free, on-policy learning.\n",
    "- **Dyna-Q** - model-based RL that blends learning and planning.\n",
    "\n",
    "Each algorithm is trained to navigate a **stochastic** grid world with obstacles, pits, and a goal state.\n",
    "\n",
    "---\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "Before running notebooks, make sure your environment is active and functional:\n",
    "\n",
    "```bash\n",
    "source .venv/bin/activate\n",
    "jupyter lab\n",
    "```\n",
    "\n",
    "If you have not yet installed dependencies:\n",
    "```bash\n",
    "# Preferable\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# In case 'requirements.txt' is not available\n",
    "pip install numpy scipy matplotlib jupyterlab ipykernel pandas tqdm \\\n",
    "            black ruff pytest pytest-cov mypy gymnasium pygame\n",
    "\n",
    "# install packages in ieditable mode\n",
    "pip install -e .\n",
    "```\n",
    "---\n",
    "\n",
    "## Repository Structure\n",
    "\n",
    "```text\n",
    "reinforcement_learning/\n",
    "├── Start_Here.ipynb     \n",
    "├── notebooks/               ← main experiments\n",
    "|   |── 00_RL.ipynb      ← you are here!\n",
    "│   ├── 01_q_learning.ipynb\n",
    "│   ├── 02_sarsa.ipynb\n",
    "│   └── 03_dyna_q.ipynb\n",
    "├── src/rl_capstone/         ← core implementation\n",
    "│   ├── gridworld.py\n",
    "│   ├── rl_algorithms.py\n",
    "│   └── utils.py\n",
    "├── data/                    ← training logs and results\n",
    "├── figs/                    ← generated plots\n",
    "├── reports/                 ← milestone & final reports\n",
    "├── tests/                   ← unit tests\n",
    "└── README.md                ← setup and project overview\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## How to Get Started\n",
    "\n",
    "1. Open the **Q-Learning notebook**:\n",
    "   \n",
    "- [notebooks/01_q_learning.ipynb](notebooks/01_q_learning.ipynb)\n",
    "\n",
    "2. Run all cells top-to-bottom (**Shift + Enter**) to train a Q-learning agent.\n",
    "\n",
    "3. Explore other algorithms:\n",
    "- [SARSA notebook](notebooks/02_sarsa.ipynb)\n",
    "- [Dyna-Q notebook](notebooks/03_dyna_q.ipynb)\n",
    "\n",
    "4. Compare results using plots saved under the **figs/** directory\n",
    "\n",
    "---\n",
    "\n",
    "## Notes\n",
    "\n",
    "- All algorithm implementations live in the **src/rl_capstone/** folder.\n",
    "- The GridWorld environment defines states, transitions, and rewards.\n",
    "- You can modify hyperparameters (**alpha**, **gamma**, **epsilon**, etc) directly in each notebook to experiment.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "- Start with **Q-Learning** to understand the training loop.\n",
    "- Proceed to **SARSA** to compare on-policy learning.\n",
    "- Explore **Dyna-Q** to see how planning accelerates learning.\n",
    "- Document your results and insights in the **report/** folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf421b4-e8f1-471e-8dcc-4204f4d76e98",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<style>\n",
    "    .button {\n",
    "        background-color: #3b3b3b;\n",
    "        color: white;\n",
    "        padding: 25px 60px;\n",
    "        border: none;\n",
    "        border-radius: 12px;\n",
    "        cursor: pointer;\n",
    "        font-size: 30px;\n",
    "        transition: background-color 0.3s ease;\n",
    "    }\n",
    "\n",
    "    .button:hover {\n",
    "        background-color: #45a049;\n",
    "        transform: scale(1.05);\n",
    "    }\n",
    "    \n",
    "</style>\n",
    "\n",
    "<div style=\" text-align: center; margin-top:20px;\">\n",
    "    \n",
    "  <a href=\"../Start_Here.ipynb\">\n",
    "    <button class=\"button\">\n",
    "      ⬅️ Prev: Start Here\n",
    "    </button>\n",
    "  </a>\n",
    "  <span style=\"display:inline-block; width:200px;\"></span>\n",
    "  <a href=\"01_q_learning.ipynb\">\n",
    "    <button class=\"button\">\n",
    "      Next: Q-Learning ➡️\n",
    "    </button>\n",
    "  </a>\n",
    "  \n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
