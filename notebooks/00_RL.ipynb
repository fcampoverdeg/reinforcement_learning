{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41b8fd0c-2555-4a14-ab5d-4eaab2fc4e01",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\"line-height: 1.7;\">\n",
    "    <h2 style=\"font-weight: 600;\"><strong>Welcome to the Reinforcement Learning Project</strong></h2>\n",
    "</div> \n",
    "\n",
    "&nbsp;\n",
    "\n",
    "This notebook serves as a quick **guide and orientation** for anyone opening this repository for the first time.\n",
    "It explains what the project is about, how the files are organized, and how to get started running experiments in **JupyterLab**.\n",
    "\n",
    "\n",
    "\n",
    "By: Felipe Campoverde\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1de282-f9a0-452e-ae2b-82d63479a313",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Project Overview\n",
    "\n",
    "Reinforcement Learning (RL) is a branch of Machine Learning that focuses on **how agents learn to make decisions** through interacting with an environment.\n",
    "The agent receives **rewards or penalties** based on its actions and gradually learns a policy that **maximizes cumulative reward** over time.\n",
    "\n",
    "In this project, the student implemented and compared several RL algorithms using a **custome GridWorld environment**:\n",
    "\n",
    "- **Q-Learning** - model-free, off-policy learning.\n",
    "- **SARSA** - model-free, on-policy learning.\n",
    "- **Dyna-Q** - model-based RL that blends learning and planning.\n",
    "\n",
    "Each algorithm is trained to navigate a **stochastic** grid world with obstacles, pits, and a goal state.\n",
    "\n",
    "---\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "Before running notebooks, make sure your environment is active and functional:\n",
    "\n",
    "```bash\n",
    "source .venv/bin/activate\n",
    "jupyter lab\n",
    "```\n",
    "\n",
    "If you have not yet installed dependencies:\n",
    "```bash\n",
    "# Preferable\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# In case 'requirements.txt' is not available\n",
    "pip install numpy scipy matplotlib jupyterlab ipykernel pandas tqdm \\\n",
    "            black ruff pytest pytest-cov mypy gymnasium pygame\n",
    "\n",
    "# install packages in editable mode\n",
    "pip install -e .\n",
    "\n",
    "# Run Unit Tests\n",
    "pytest -q\n",
    "```\n",
    "---\n",
    "\n",
    "## Repository Structure\n",
    "\n",
    "```text\n",
    "reinforcement_learning/\n",
    "├── Start_Here.ipynb     \n",
    "├── notebooks/               ← main experiments\n",
    "|   |── 00_RL.ipynb      ← You are here!\n",
    "│   ├── 01_q_learning.ipynb\n",
    "│   ├── 02_sarsa.ipynb\n",
    "│   ├── 03_dyna_q.ipynb\n",
    "│   ├── 04_comparison_models.ipynb\n",
    "│   ├── 05_k_sweep.ipynb\n",
    "│   ├── 06_robustness.ipynb\n",
    "│   └── 07_results.ipynb\n",
    "│ \n",
    "├── src/rl_capstone/         ← core implementation\n",
    "│   ├── gridworld.py\n",
    "│   ├── rl_algorithms.py\n",
    "│   └── utils.py\n",
    "│ \n",
    "├── data/                    ← training logs and results\n",
    "├── figs/                    ← generated plots\n",
    "├── reports/                 ← milestone & final reports\n",
    "├── tests/                   ← unit tests\n",
    "└── README.md                ← setup and project overview\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## How to Get Started\n",
    "\n",
    "1. Open the **Q-Learning notebook**:\n",
    "   \n",
    "- [Q-Learning](notebooks/01_q_learning.ipynb)\n",
    "\n",
    "2. Run all cells top-to-bottom (**Shift + Enter**) to train the agent.\n",
    "\n",
    "3. Explore other algorithms:\n",
    "- [SARSA notebook](notebooks/02_sarsa.ipynb)\n",
    "- [Dyna-Q notebook](notebooks/03_dyna_q.ipynb)\n",
    "\n",
    "4. Compare results from algorithms:  \n",
    "[Comparing Models](notebooks/04_comparison_models.ipynb)\n",
    "\n",
    "5. Analyze differences in Dyna-Q behaviour under different tests:  \n",
    "- [K Sweeping Dyna-Q](notebooks/05_k_sweep.ipynb)\n",
    "- [Robustness & Generalization](notebooks/06_robustness.ipynb)\n",
    "\n",
    "6. Results:  \n",
    "- [Results](notebooks/07_results.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Notes\n",
    "\n",
    "- All algorithm implementations live in the **src/rl_capstone/** folder.\n",
    "- The GridWorld environment defines states, transitions, and rewards.\n",
    "- You can modify hyperparameters (**alpha**, **gamma**, **epsilon**, etc) directly in each notebook to experiment.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "- Start with **Q-Learning** to understand the training loop.\n",
    "- Proceed to **SARSA** to compare on-policy learning.\n",
    "- Explore **Dyna-Q** to see how planning accelerates learning.\n",
    "- Test different case scenarios using Dyna-Q to learn changes on its behavior.\n",
    "- Document your results and insights in the **result** notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf421b4-e8f1-471e-8dcc-4204f4d76e98",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<style>\n",
    "    .button {\n",
    "        background-color: #3b3b3b;\n",
    "        color: white;\n",
    "        padding: 25px 60px;\n",
    "        border: none;\n",
    "        border-radius: 12px;\n",
    "        cursor: pointer;\n",
    "        font-size: 30px;\n",
    "        transition: background-color 0.3s ease;\n",
    "    }\n",
    "\n",
    "    .button:hover {\n",
    "        background-color: #45a049;\n",
    "        transform: scale(1.05);\n",
    "    }\n",
    "    \n",
    "</style>\n",
    "\n",
    "<div style=\" text-align: center; margin-top:20px;\">\n",
    "    \n",
    "  <a href=\"../Start_Here.ipynb\">\n",
    "    <button class=\"button\">\n",
    "      ⬅️ Prev: Start Here\n",
    "    </button>\n",
    "  </a>\n",
    "  <span style=\"display:inline-block; width:200px;\"></span>\n",
    "  <a href=\"01_q_learning.ipynb\">\n",
    "    <button class=\"button\">\n",
    "      Next: Q-Learning ➡️\n",
    "    </button>\n",
    "  </a>\n",
    "  \n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
