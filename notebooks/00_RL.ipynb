{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "682fa012-c6ec-4850-b6e0-5f0323366574",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\"line-height: 1.6;\">\n",
    "\n",
    "<h2 style=\"fond-weight: 800;\"><strong>Grid World Reinforcement Learning</h2>\n",
    "\n",
    "<h4 style=\"text=align: center; font-weight: 500; font-size: 20px; font-style: italic;\">Model-Based vs. Model-Free Reinforcement Learning in a Stochastic GridWorld: Sample Efficiency and Robustness</h4>\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<p style=\"text-align: center; margin-top: 15px; font-size: 20px;\">Felipe S. Campoverde</p>\n",
    "\n",
    "<p  style=\"text-align: center; font-size: 18px; \">\n",
    "CS 4824 Machine Learning &nbsp; . &nbsp;\n",
    "Instructor: Dr. Ming Jin &nbsp; . &nbsp;\n",
    "Date: 09/19/2025\n",
    "</p>\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c833e6df-f4c8-43e1-97ab-8019eb07f9f7",
   "metadata": {},
   "source": [
    "## **Problem Definition**\n",
    "\n",
    "This project investigates how an agent can learn effective behavior purely via trial–and–error in\n",
    "a compact 2D environment. The student will design a stochastic GridWorld with walls, pits, and\n",
    "wind (transition noise) and compare model-free RL (Q-learning, SARSA) against model-based RL\n",
    "via Dyna-Q, which interleaves learning a one-step model with planning updates.\n",
    "\n",
    "**Motivation & Significance**. GridWorlds isolate core RL phenomena, exploration, boot-\n",
    "strapping, on- vs. off-policy learning without heavy computation. Understanding when planning\n",
    "helps and how hyperparameters affect sample efficiency provides practical guidance for scaling RL\n",
    "to richer problems.\n",
    "\n",
    "**Research Questions.**\n",
    "1. How do planning steps (**K**) in **Dyna-Q** affect sample efficiency (return vs. environment steps)\n",
    "under stochastic dynamics?\n",
    "2. How sensitive are **Q-learning**, **SARSA**, and **Dyna-Q** to exploration schedules ( ε-greedy\n",
    "with/without decay) and learning rates?\n",
    "3. How robust are learned policies to layout changes (added obstacles) and transition noise (wind\n",
    "strength)?\n",
    "4. Does prioritized sweeping further improve efficiency compared to vanilla **Dyna-Q**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ade46f0-404f-4829-866a-4ca0787f366f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## **Literature Review**\n",
    "Foundational work establishes the methods to be compared: Q-learning provides off-policy TD\n",
    "control with convergence guarantees in tabular settings [2]. SARSA offers an on-policy counterpart\n",
    "with different exploration–exploitation behavior [3, 5]. Dyna integrates learning and planning by\n",
    "updating from both real experience and a learned model [1]; prioritized sweeping improves planning\n",
    "by focusing backups where they matter most [4]. Sutton and Barto [5] synthesize these approaches.\n",
    "The project addresses a practical gap: a controlled, reproducible comparison of model-free vs.\n",
    "model-based methods under stochasticity and layout shift, with thorough ablations on planning\n",
    "budget and exploration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
