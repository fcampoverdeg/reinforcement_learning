{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a093a020-ed99-4bbd-a53f-84b21328b521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS: \n",
    "# This setup is important so every notebook knows where to find 'rl_capstone'\n",
    "\n",
    "# ---- Setup ---- #\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "\n",
    "# ---- Imports ---- #\n",
    "from rl_capstone.gridworld import GridWorld, GridWorldConfig\n",
    "from rl_capstone.rl_algorithms import q_learning\n",
    "from rl_capstone.utils import epsilon_greedy\n",
    "\n",
    "# ---- Quick Test ---- #\n",
    "env = GridWorld(GridWorldConfig())\n",
    "env.render()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1de282-f9a0-452e-ae2b-82d63479a313",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Welcome to the Reinforcement Learning Project\n",
    "\n",
    "This notebook serves as a quick **guide and orientation** for anyone opening this repository for the first time.\n",
    "It explains what the project is about, how the files are organized, and how to get started running experiments in **JupyterLab**.\n",
    "\n",
    "\n",
    "\n",
    "By: Felipe Campoverde\n",
    "\n",
    "---\n",
    "\n",
    "# Project Overview\n",
    "\n",
    "Reinforcement Learning (RL) is a branch of Machine Learning that focuses on **how agents learn to make decisions** through interacting with an environment.\n",
    "The agent receives **rewards or penalties** based on its actions and gradually learns a policy that **maximizes cumulative reward** over time.\n",
    "\n",
    "In this project, the student implemented and compared several RL algorithms using a **custome GridWorld environment**:\n",
    "\n",
    "- **Q-Learning** - model-free, off-policy learning.\n",
    "- **SARSA** - model-free, on-policy learning.\n",
    "- **Dyna-Q** - model-based RL that blends learning and planning.\n",
    "\n",
    "Each algorithm is trained to navigate a **stochastic** grid world with obstacles, pits, and a goal state.\n",
    "\n",
    "---\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "Before running notebooks, make sure your environment is active and functional:\n",
    "\n",
    "```bash\n",
    "source .venv/bin/activate\n",
    "jupyter lab\n",
    "```\n",
    "\n",
    "If you have not yet installed dependencies:\n",
    "```bash\n",
    "# Preferable\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# In case 'requirements.txt' is not available\n",
    "pip install numpy scipy matplotlib jupyterlab ipykernel pandas tqdm \\\n",
    "            black ruff pytest pytest-cov mypy gymnasium pygame\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Repository Structure\n",
    "\n",
    "```text\n",
    "reinforcement_learning/\n",
    "├── Start_Here.ipynb     \n",
    "├── notebooks/               ← main experiments\n",
    "|   |── 00_RL.ipynb      ← you are here!\n",
    "│   ├── 01_q_learning.ipynb\n",
    "│   ├── 02_sarsa.ipynb\n",
    "│   └── 03_dyna_q.ipynb\n",
    "├── src/rl_capstone/         ← core implementation\n",
    "│   ├── gridworld.py\n",
    "│   ├── rl_algorithms.py\n",
    "│   └── utils.py\n",
    "├── data/                    ← training logs and results\n",
    "├── figs/                    ← generated plots\n",
    "├── reports/                 ← milestone & final reports\n",
    "├── tests/                   ← unit tests\n",
    "└── README.md                ← setup and project overview\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## How to Get Started\n",
    "\n",
    "1. Open the **Q-Learning notebook**:\n",
    "   \n",
    "- [notebooks/01_q_learning.ipynb](notebooks/01_q_learning.ipynb)\n",
    "\n",
    "2. Run all cells top-to-bottom (**Shift + Enter**) to train a Q-learning agent.\n",
    "\n",
    "3. Explore other algorithms:\n",
    "- [SARSA notebook](notebooks/02_sarsa.ipynb)\n",
    "- [Dyna-Q notebook](notebooks/03_dyna_q.ipynb)\n",
    "\n",
    "4. Compare results using plots saved under the **figs/** directory\n",
    "\n",
    "---\n",
    "\n",
    "## Notes\n",
    "\n",
    "- All algorithm implementations live in the **src/rl_capstone/** folder.\n",
    "- The GridWorld environment defines states, transitions, and rewards.\n",
    "- You can modify hyperparameters (**alpha**, **gamma**, **epsilon**, etc) directly in each notebook to experiment.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "- Start with **Q-Learning** to understand the training loop.\n",
    "- Proceed to **SARSA** to compare on-policy learning.\n",
    "- Explore **Dyna-Q** to see how planning accelerates learning.\n",
    "- Document your results and insights in the **report/** folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf421b4-e8f1-471e-8dcc-4204f4d76e98",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<style>\n",
    "    .button {\n",
    "        background-color: #3b3b3b;\n",
    "        color: white;\n",
    "        padding: 25px 60px;\n",
    "        border: none;\n",
    "        border-radius: 12px;\n",
    "        cursor: pointer;\n",
    "        font-size: 30px;\n",
    "        transition: background-color 0.3s ease;\n",
    "    }\n",
    "\n",
    "    .button:hover {\n",
    "        background-color: #45a049;\n",
    "        transform: scale(1.05);\n",
    "    }\n",
    "    \n",
    "</style>\n",
    "\n",
    "<div style=\" text-align: center; margin-top:20px;\">\n",
    "    \n",
    "  <a href=\"../Start_Here.ipynb\">\n",
    "    <button class=\"button\">\n",
    "      ⬅️ Prev: Start Here\n",
    "    </button>\n",
    "  </a>\n",
    "  <span style=\"display:inline-block; width:200px;\"></span>\n",
    "  <a href=\"01_q_learning.ipynb\">\n",
    "    <button class=\"button\">\n",
    "      Next: Q-Learning ➡️\n",
    "    </button>\n",
    "  </a>\n",
    "  \n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
