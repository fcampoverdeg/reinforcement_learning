{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "827def7b-7e83-409e-b6db-5da7e1069395",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\"line-height: 1.7;\">\n",
    "    <h2 style=\"font-weight: 600;\"><strong>Results & Conclusion</strong></h2>\n",
    "</div> \n",
    "\n",
    "&nbsp;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa7556a-2779-40a5-abe3-78decdf0223c",
   "metadata": {},
   "source": [
    "## Problem Definition\n",
    "\n",
    "The goal of this project is to understand how an agent can learn effective decision-making through **trial and error** in a stochastic GridWorld environment. Unlike deep RL benchmarks, GridWorld isolates core reinforcement learning phenomena (exploration, temporal difference learning, planning, bootstrapping) in a compact, fully observable domain.\n",
    "\n",
    "We compare:\n",
    "\n",
    "* **Q-Learning** (model-free, off-policy)\n",
    "* **SARSA** (model-free, on-policy)\n",
    "* **Dyna-Q** (model-based + planning via simulated updates)\n",
    "\n",
    "The environment includes **walls**, **pits**, and **wind** (transition noise). The agent must learn to reach the goal while avoiding hazards and minimizing step penalties.\n",
    "\n",
    "---\n",
    "\n",
    "## Motivation & Significance\n",
    "\n",
    "Understaind planning and robutness in tabular RL is foundational for real RL systems:\n",
    "\n",
    "* **GridWorld** lets us visualize value propagation and policy formation clearly.\n",
    "\n",
    "* **Model-based RL (Dyna-Q)** is one of the first examples of how simulated experience can vastly improve sample efficiency.\n",
    "\n",
    "* **Robustness testing** (layout shift, dynamics shift, seed stability) reflects real challenges in robotics and general purpose RL:\n",
    "  \n",
    "    * Environments change\n",
    "    \n",
    "    * Dynamics are not perfectly known\n",
    "    \n",
    "    * Algorithms may be brittle across random seeds\n",
    "\n",
    "Studying these effects in a simple controlled setting provides insight applicable to larger scale RL systems.\n",
    "\n",
    "---\n",
    "\n",
    "## Research Questions & Answers\n",
    "\n",
    "**How do planning steps ($K$) in Dyna-Q affect sample efficiency?**\n",
    "\n",
    "Answer:  \n",
    "Planning dramatically accelerates learning **up to a point**. The sweep revealed:\n",
    "\n",
    "* **$K=0$** (pure model-free) -> slowest learning (~190 episodes to reach >= 0.6 return)\n",
    "  \n",
    "* **$K=5-20$** -> fastest learning (~90-110 episodes).\n",
    "\n",
    "* **$K-50$** -> worse than $K=10$ or $K=20$; diminishing returns and slight instability.\n",
    "\n",
    "However, all K values converged to the **same final optimal policy**. So planning controls speed, not optimality.\n",
    "\n",
    "---\n",
    "\n",
    "**How sensitive are the algorithms to ε-greedy schedules and learning rates?**\n",
    "\n",
    "Answer:\n",
    "All algorithms converge robustly under reasonable $\\alpha$ and $\\epsilon$ schedules, but:\n",
    "\n",
    "* Q-Learning is the most sensitive to high $\\epsilon$ early in training (off-policy bootstrapping amplifies exploratory randomness).\n",
    "\n",
    "* SARSA is the most stable accross schedules (on-policy learning compensates for poor exploration choices).\n",
    "\n",
    "* Dyna-Q is the most sensitive to $\\alpha$ and $\\epsilon$ because planning amplifies whatever trajectories are experienced early.\n",
    "\n",
    "Overall, **SARSA is consistently the least sensitive** to hyperparameter changes.\n",
    "\n",
    "---\n",
    "\n",
    "**How robust are policies to layout changes and transition noise?**\n",
    "\n",
    "To answer this question, we examined three ways:\n",
    "\n",
    "**1. Dynamics Shift (Windy Environment)**\n",
    "\n",
    "* All algorithms adapt well; final returns are similar (~0.63-0.70).\n",
    "  \n",
    "* Dyna-Q learns **faster** but final performance is slightly worse.\n",
    "\n",
    "* Seed stability is extremely high across all three.\n",
    "\n",
    "**2. Layout Shift (New walls/pits)**  \n",
    "Two tests:\n",
    "\n",
    "  **Cross-evaluation (no retraining)**:  \n",
    "Policies learned on the baseline grid, evaluated on the shifted one.\n",
    "\n",
    "   * **SARSA**: remains positive (+0.52)\n",
    "\n",
    "   * **Q-Learning & Dyna-Q**: collapse (-0.96 to -1.02)\n",
    "\n",
    "SARSA clearly generalizes better across layouts.\n",
    "\n",
    "**Retraining on the shifted layout:**  \n",
    "All three converge to similar final performance (~0.71 return), but:\n",
    "\n",
    "   * **Dyna-Q shows very high variance across seeds** (std ~ 0.33)\n",
    "   \n",
    "   * Q-learning and SARSA are far more consistent\n",
    "\n",
    "\n",
    "**Conclusion**\n",
    "Model-based planning makes Dyna-Q less robust to structural changes.\n",
    "\n",
    "---\n",
    "\n",
    "**Does Dyna-Q's planning provide lasting benefits?**\n",
    "\n",
    "Answer:\n",
    "Only during early learning.  \n",
    "Once the agent gathers enough real experience, all algorithms converge to the same final policy.\n",
    "\n",
    "However:  \n",
    "\n",
    "* Under layout shift, Dyna-Q is **least stable across seeds**\n",
    "\n",
    "* Under dynamics shift, planning helps early but does not yield a better final policy.\n",
    "\n",
    "**Overall**:\n",
    "Planning accelerates learning but reduces robustness.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary of Key Findings\n",
    "\n",
    "**Learning Efficiency**  \n",
    "\n",
    "* Dyna-Q with moderate $K (10-20)$ is the fastest learner.\n",
    "  \n",
    "* Too much planning ($K=50$) can make learning worse.\n",
    "\n",
    "\n",
    "**Final Performance**\n",
    "\n",
    "* All algorithms reach identical optimal performance when trained in the same environment.\n",
    "\n",
    "\n",
    "**Robustness**\n",
    "\n",
    "* **SARSA** is most robust to layout shift and hyperparameter sensitivity.\n",
    "\n",
    "* **Q-Learning** is moderately stable.\n",
    "\n",
    "* **Dyna-Q** shows the worst generalization under structural changes.\n",
    "\n",
    "\n",
    "**Seed Stability**\n",
    "\n",
    "* Windy env -> very stable for all.\n",
    "\n",
    "* Layout shift -> Dyna-Q highly unstable across seeds.\n",
    "\n",
    "---\n",
    "\n",
    "## Overall Conclusion\n",
    "\n",
    "This project highlights the classic Dyna-Q trade-off:\n",
    "\n",
    "   **Planning accelerates learning but amplifies early mistakes, reducing robustness.**\n",
    "\n",
    "In simple stationary environments, model-based RL is a major win: fast learning and optimal asymptoti performance.\n",
    "\n",
    "But under realistic conditions (changing layouts, noisy transitions), **model-free methods especially SARSA are significancly more robust** and consistent.\n",
    "\n",
    "These results reinforce a theme seen in modern RL research:\n",
    "\n",
    "**better sample efficiency does not guarantee better generalization.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a33adfc-62b7-421d-942e-a1d71483f5cb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<style>\n",
    "    .button {\n",
    "        background-color: #3b3b3b;\n",
    "        color: white;\n",
    "        padding: 25px 60px;\n",
    "        border: none;\n",
    "        border-radius: 12px;\n",
    "        cursor: pointer;\n",
    "        font-size: 30px;\n",
    "        transition: background-color 0.3s ease;\n",
    "    }\n",
    "\n",
    "    .button:hover {\n",
    "        background-color: #45a049;\n",
    "        transform: scale(1.05);\n",
    "    }\n",
    "    \n",
    "</style>\n",
    "\n",
    "<div style=\" text-align: center; margin-top:20px;\">\n",
    "    \n",
    "  <a href=\"06_robustness.ipynb\">\n",
    "    <button class=\"button\">\n",
    "      ⬅️ Prev: Robustness & Generalization\n",
    "    </button>\n",
    "  </a>\n",
    "  <span style=\"display:inline-block; width:200px;\"></span>\n",
    "  <a href=\"../Start_Here.ipynb\">\n",
    "    <button class=\"button\">\n",
    "      Next: Start Here ➡️\n",
    "    </button>\n",
    "  </a>\n",
    "  \n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
